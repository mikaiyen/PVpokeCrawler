from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from datetime import datetime
from git import Repo
import shutil
import threading
import tempfile
from pathlib import Path
import uuid
import psutil

# è‡ªå‹•æ¸…é™¤å£æ‰çš„ .wdm cache
cache_dir = os.path.join(os.path.expanduser("~"), ".wdm")
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print("å·²æ¸…é™¤ .wdm cacheï¼Œç­‰å¾…é‡æ–°ä¸‹è¼‰ä¹¾æ·¨çš„ driver")

# å®šç¾©æ‰€æœ‰å±¬æ€§çˆ¬èŸ²ä»»å‹™
POKEMON_TYPES = ["normal", "fire", "water", "electric", "grass", "ice", "fighting", 
                "poison", "ground", "flying", "psychic", "bug", "rock", "ghost", 
                "dragon", "dark", "steel", "fairy"]

# ç”Ÿæˆçˆ¬èŸ²ä»»å‹™åˆ—è¡¨
CRAWLER_TASKS = []
for i, ptype in enumerate(POKEMON_TYPES):
    CRAWLER_TASKS.append({
        "crawler_id": f"Crawler-{ptype}",
        "filename": f"{ptype}.csv",
        "url": f"https://db.pokemongohub.net/pokemon-list/best-per-type/{ptype}",
        "debug_port": 9222 + i,  # æ¯å€‹çˆ¬èŸ²ä½¿ç”¨ä¸åŒç«¯å£
        "type": ptype
    })

# å…¨åŸŸè®Šæ•¸å’Œé–
driver_lock = threading.Lock()
driver_path = None

def get_driver_path():
    """ç·šç¨‹å®‰å…¨åœ°ç²å– ChromeDriver è·¯å¾‘"""
    global driver_path
    with driver_lock:
        if driver_path is None:
            print("æ­£åœ¨ä¸‹è¼‰å’Œå®‰è£ ChromeDriver...")
            driver_path = ChromeDriverManager().install()
            print(f"ChromeDriver å®‰è£å®Œæˆ: {driver_path}")
            
            # ç¢ºä¿æª”æ¡ˆæ¬Šé™æ­£ç¢º
            try:
                os.chmod(driver_path, 0o755)
            except Exception as e:
                print(f"è¨­å®š ChromeDriver æ¬Šé™æ™‚ç™¼ç”Ÿè­¦å‘Š: {e}")
        
        return driver_path

def create_unique_user_data_dir(crawler_id):
    """ç‚ºæ¯å€‹çˆ¬èŸ²å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„"""
    temp_dir = tempfile.gettempdir()
    unique_dir = os.path.join(temp_dir, f"chrome_profile_{crawler_id}_{uuid.uuid4().hex[:8]}")
    os.makedirs(unique_dir, exist_ok=True)
    return unique_dir

def setup_driver(crawler_id, debug_port):
    """ç‚ºæŒ‡å®šçš„çˆ¬èŸ²è¨­å®š Selenium WebDriver"""
    print(f"[{crawler_id}] æ­£åœ¨åˆå§‹åŒ– WebDriver...")
    
    # å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„
    user_data_dir = create_unique_user_data_dir(crawler_id)
    
    options = webdriver.ChromeOptions()
    
    # åŸºæœ¬é¸é …
    options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„ headless æ¨¡å¼
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    
    # ç‚ºæ¯å€‹çˆ¬èŸ²æŒ‡å®šç¨ç«‹è³‡æº
    options.add_argument(f"--user-data-dir={user_data_dir}")
    options.add_argument(f"--remote-debugging-port={debug_port}")
    
    # æ€§èƒ½å„ªåŒ–é¸é …
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    options.add_argument("--disable-ipc-flooding-protection")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    options.add_argument("--memory-pressure-off")
    options.add_argument("--max_old_space_size=4096")
    
    # ç²å– ChromeDriver è·¯å¾‘
    chrome_driver_path = get_driver_path()
    
    driver = None
    try:
        service = Service(chrome_driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(10)
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ç«¯å£: {debug_port}")
        return driver, user_data_dir
        
    except Exception as e:
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
        if driver:
            try:
                driver.quit()
            except:
                pass
        # æ¸…ç†è³‡æº
        try:
            shutil.rmtree(user_data_dir, ignore_errors=True)
        except:
            pass
        raise

def cleanup_crawler_resources(driver, user_data_dir, crawler_id):
    """æ¸…ç†çˆ¬èŸ²ç›¸é—œè³‡æº"""
    if driver:
        try:
            driver.quit()
            print(f"[{crawler_id}] WebDriver å·²é—œé–‰")
        except Exception as e:
            print(f"[{crawler_id}] é—œé–‰ WebDriver æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    try:
        shutil.rmtree(user_data_dir, ignore_errors=True)
        print(f"[{crawler_id}] å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„: {user_data_dir}")
    except Exception as e:
        print(f"[{crawler_id}] æ¸…ç†è‡¨æ™‚ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def kill_chrome_processes():
    """æ¸…ç†æ®˜ç•™çš„ Chrome é€²ç¨‹"""
    try:
        killed_count = 0
        for proc in psutil.process_iter(['pid', 'name']):
            if 'chrome' in proc.info['name'].lower():
                try:
                    proc.kill()
                    killed_count += 1
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        if killed_count > 0:
            print(f"å·²çµ‚æ­¢ {killed_count} å€‹ Chrome é€²ç¨‹")
    except Exception as e:
        print(f"æ¸…ç† Chrome é€²ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def run_crawler(task):
    """é‹è¡Œå–®å€‹çˆ¬èŸ²ä»»å‹™"""
    crawler_id = task["crawler_id"]
    filename = task["filename"]
    url = task["url"] 
    debug_port = task["debug_port"]
    ptype = task["type"]
    
    max_retries = 3
    
    for attempt in range(max_retries):
        driver = None
        user_data_dir = None
        
        try:
            print(f"[{crawler_id}] é–‹å§‹çˆ¬å– {ptype} å±¬æ€§è³‡æ–™ (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            # åˆå§‹åŒ– WebDriver
            driver, user_data_dir = setup_driver(crawler_id, debug_port)
            
            # è¼‰å…¥é é¢
            print(f"[{crawler_id}] æ­£åœ¨è¼‰å…¥é é¢: {url}")
            driver.get(url)
            time.sleep(5)  # é¡å¤–ç­‰å¾…æ™‚é–“
            
            # 1ï¸âƒ£ å˜—è©¦å¤šç¨® selectorï¼Œæ‰¾åˆ°é é¢ä¸»è¦å…§å®¹
            print(f"[{crawler_id}] ç­‰å¾…é é¢å…§å®¹è¼‰å…¥...")
            try:
                # å…ˆå˜—è©¦åŸæœ¬çš„é¸æ“‡å™¨
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located(
                        (By.XPATH, '//div[contains(@class,"layout_layout__")]')
                    )
                )
                print(f"[{crawler_id}] æ‰¾åˆ° layout å…ƒç´ ")
            except:
                print(f"[{crawler_id}] æ‰¾ä¸åˆ° layoutï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨...")
                # å˜—è©¦å…¶ä»–å¯èƒ½çš„é¸æ“‡å™¨
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located(
                            (By.TAG_NAME, "main")
                        )
                    )
                    print(f"[{crawler_id}] æ‰¾åˆ° main å…ƒç´ ")
                except:
                    print(f"[{crawler_id}] å˜—è©¦ç­‰å¾… body è¼‰å…¥å®Œæˆ...")
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located(
                            (By.TAG_NAME, "body")
                        )
                    )
                    time.sleep(3)  # é¡å¤–ç­‰å¾…æ™‚é–“

            # 2ï¸âƒ£ å¦‚æœæœ‰ cookie å½ˆçª—ï¼Œå…ˆé—œæ‰
            try:
                cookie_btn = WebDriverWait(driver, 3).until(
                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(),"Accept")]'))
                )
                cookie_btn.click()
                print(f"[{crawler_id}] å·²é»æ“Š Accept cookies")
                time.sleep(1)
            except:
                print(f"[{crawler_id}] æ²’æœ‰æ‰¾åˆ° cookie å½ˆçª—")

            # 3ï¸âƒ£ ç­‰çµæœå€å¡Šè¼‰å…¥ï¼Œå˜—è©¦å¤šç¨®é¸æ“‡å™¨
            print(f"[{crawler_id}] å°‹æ‰¾çµæœå€å¡Š...")
            results = None
            
            # å˜—è©¦ä¸åŒçš„é¸æ“‡å™¨
            selectors = [
                '//div[contains(@class,"PokemonCounters_results__")]',
                '//div[contains(@class,"results")]',
                '//div[contains(@class, "pokemon")]',
                '//main//div',
                '//div[@class]'
            ]
            
            for selector in selectors:
                try:
                    results = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.XPATH, selector))
                    )
                    print(f"[{crawler_id}] æ‰¾åˆ°çµæœå€å¡Š: {selector}")
                    break
                except:
                    continue
            
            if results is None:
                print(f"[{crawler_id}] æ‰€æœ‰é¸æ“‡å™¨éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥å¾æ•´å€‹é é¢æŠ“å–...")
                results = driver.find_element(By.TAG_NAME, "body")

            # 4ï¸âƒ£ æŠ“å–å¯¶å¯å¤¢åç¨±ï¼Œå˜—è©¦å¤šç¨®æ–¹å¼
            print(f"[{crawler_id}] æŠ“å–å¯¶å¯å¤¢åç¨±...")
            names = []
            
            # æ–¹æ³•1: åŸæœ¬çš„æ–¹å¼
            try:
                name_elems = results.find_elements(By.XPATH, './/a[contains(@href,"/pokemon/")]')
                names = [e.text.strip() for e in name_elems if e.text.strip()]
                print(f"[{crawler_id}] æ–¹æ³•1æ‰¾åˆ° {len(names)} å€‹åç¨±")
            except:
                print(f"[{crawler_id}] æ–¹æ³•1å¤±æ•—")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²’æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            if len(names) == 0:
                try:
                    name_elems = driver.find_elements(By.XPATH, '//a[contains(@href,"pokemon")]')
                    names = [e.text.strip() for e in name_elems if e.text.strip()]
                    print(f"[{crawler_id}] æ–¹æ³•2æ‰¾åˆ° {len(names)} å€‹åç¨±")
                except:
                    print(f"[{crawler_id}] æ–¹æ³•2å¤±æ•—")
            
            # æ–¹æ³•3: æ›´å»£æ³›çš„æœå°‹
            if len(names) == 0:
                try:
                    # å˜—è©¦æ‰¾ä»»ä½•åŒ…å«pokemonçš„é€£çµ
                    name_elems = driver.find_elements(By.PARTIAL_LINK_TEXT, "")
                    all_links = [e.get_attribute('href') for e in name_elems if e.get_attribute('href')]
                    pokemon_links = [link for link in all_links if 'pokemon' in link.lower()]
                    print(f"[{crawler_id}] æ‰¾åˆ° {len(pokemon_links)} å€‹åŒ…å«pokemonçš„é€£çµ")
                    
                    if len(pokemon_links) > 0:
                        # å¾é€£çµä¸­æå–å¯¶å¯å¤¢åç¨±
                        for link in pokemon_links[:50]:  # é™åˆ¶50å€‹
                            try:
                                # å¾URLä¸­æå–å¯¶å¯å¤¢åç¨±
                                name = link.split('/')[-1].replace('-', ' ').title()
                                if name:
                                    names.append(name)
                            except:
                                continue
                        print(f"[{crawler_id}] å¾é€£çµæå–åˆ° {len(names)} å€‹åç¨±")
                        
                except Exception as e:
                    print(f"[{crawler_id}] æ–¹æ³•3å¤±æ•—: {e}")

            # å–å‰50å€‹çµæœ
            names = names[:50] if names else []
            
            if not names:
                print(f"[{crawler_id}] è­¦å‘Š: {ptype} å±¬æ€§æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™")
                if attempt < max_retries - 1:
                    continue
                else:
                    print(f"[{crawler_id}] {ptype} å±¬æ€§çˆ¬å–å¤±æ•—ï¼šæ‰¾ä¸åˆ°è³‡æ–™")
                    return False
            
            # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
            if not os.path.exists("pve"):
                os.makedirs("pve")
            
            # å„²å­˜æˆ CSV æª”æ¡ˆ
            data = pd.DataFrame({"rank": range(1, len(names) + 1), "name": names})
            data.to_csv(f"pve/{filename}", index=False, encoding="utf-8-sig")
            print(f"[{crawler_id}] âœ… {ptype} å±¬æ€§è³‡æ–™å·²æˆåŠŸæŠ“å–ä¸¦å„²å­˜ (å…± {len(names)} ç­†è³‡æ–™)")
            return True
            
        except Exception as e:
            print(f"[{crawler_id}] âŒ çˆ¬å– {ptype} å±¬æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"[{crawler_id}] {ptype} å±¬æ€§çˆ¬å–å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
            else:
                time.sleep(3)  # ç­‰å¾…å¾Œé‡è©¦
        finally:
            # æ¸…ç†è³‡æº
            if driver or user_data_dir:
                cleanup_crawler_resources(driver, user_data_dir, crawler_id)
    
    return False

def main():
    """ä¸»ç¨‹å¼ - å¤šå€‹çˆ¬èŸ²ä¸¦è¡ŒåŸ·è¡Œ"""
    print("=" * 80)
    print(f"å•Ÿå‹• {len(POKEMON_TYPES)} å€‹ä¸¦è¡Œçˆ¬èŸ²ï¼Œçˆ¬å–æ‰€æœ‰å¯¶å¯å¤¢å±¬æ€§çš„PVEè³‡æ–™")
    print("=" * 80)
    
    # é å…ˆæ¸…ç†å¯èƒ½æ®˜ç•™çš„ Chrome é€²ç¨‹
    print("æ¸…ç†æ®˜ç•™çš„ Chrome é€²ç¨‹...")
    kill_chrome_processes()
    
    # é å…ˆå®‰è£ ChromeDriverï¼ˆé¿å…ç«¶çˆ­ï¼‰
    print("é å…ˆæº–å‚™ ChromeDriver...")
    get_driver_path()
    
    # é¡¯ç¤ºä»»å‹™åˆ†é…
    print("\nä»»å‹™åˆ†é…:")
    for i, task in enumerate(CRAWLER_TASKS):
        print(f"  {task['crawler_id']} -> {task['type']} å±¬æ€§ (ç«¯å£: {task['debug_port']})")
        if (i + 1) % 3 == 0:  # æ¯3å€‹æ›è¡Œ
            print()
    
    print(f"\né–‹å§‹ä¸¦è¡ŒåŸ·è¡Œ {len(CRAWLER_TASKS)} å€‹çˆ¬èŸ²ä»»å‹™...")
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œï¼Œæ¯å€‹ç·šç¨‹è™•ç†ä¸€å€‹ä»»å‹™
    # é™åˆ¶åŒæ™‚åŸ·è¡Œçš„ç·šç¨‹æ•¸é‡ï¼Œé¿å…è³‡æºä¸è¶³
    max_workers = min(6, len(CRAWLER_TASKS))  # æœ€å¤š6å€‹åŒæ™‚åŸ·è¡Œ
    
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="PVE-Crawler") as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(run_crawler, task): task["crawler_id"] 
            for task in CRAWLER_TASKS
        }
        
        # ç­‰å¾…å®Œæˆä¸¦è™•ç†çµæœ
        results = {}
        completed_count = 0
        
        for future in as_completed(futures):
            crawler_id = futures[future]
            try:
                success = future.result()
                results[crawler_id] = success
                completed_count += 1
                
                if success:
                    print(f"ğŸ‰ {crawler_id} ä»»å‹™å®Œæˆ ({completed_count}/{len(CRAWLER_TASKS)})")
                else:
                    print(f"âš ï¸  {crawler_id} ä»»å‹™å¤±æ•— ({completed_count}/{len(CRAWLER_TASKS)})")
            except Exception as e:
                print(f"âŒ {crawler_id} åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[crawler_id] = False
                completed_count += 1
    
    # çµ±è¨ˆçµæœ
    success_count = sum(results.values())
    total_count = len(CRAWLER_TASKS)
    
    print("\n" + "=" * 80)
    print("çˆ¬å–çµæœç¸½è¦½:")
    
    # æŒ‰å±¬æ€§åˆ†çµ„é¡¯ç¤ºçµæœ
    for i, (crawler_id, success) in enumerate(results.items()):
        ptype = crawler_id.replace("Crawler-", "")
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        print(f"  {ptype.ljust(10)}: {status}")
        if (i + 1) % 6 == 0:  # æ¯6å€‹æ›è¡Œ
            print()
    
    print(f"\nç¸½è¨ˆ: {success_count}/{total_count} å€‹å±¬æ€§æˆåŠŸå®Œæˆ")
    print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    print("=" * 80)
    
    # æœ€çµ‚æ¸…ç†
    print("åŸ·è¡Œæœ€çµ‚æ¸…ç†...")
    time.sleep(2)
    kill_chrome_processes()
    
    # å¯é¸ï¼šæ¨é€åˆ° GitHub
    # push_to_github()

def push_to_github():
    """å°‡æ›´æ–°çš„æª”æ¡ˆæ¨é€åˆ° GitHub"""
    try:
        repo = Repo(os.getcwd())
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
        if repo.is_dirty() or repo.untracked_files:
            # æ·»åŠ æ‰€æœ‰ pve è³‡æ–™å¤¾ä¸‹çš„ CSV æª”æ¡ˆ
            pve_files = [f"pve/{ptype}.csv" for ptype in POKEMON_TYPES]
            repo.git.add(pve_files)
            repo.index.commit(f"è‡ªå‹•æ›´æ–° PVE å±¬æ€§è³‡æ–™ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            origin = repo.remote(name='origin')
            origin.push()
            print("å·²æ¨é€æ›´æ–°åˆ° GitHub")
        else:
            print("æ²’æœ‰æª”æ¡ˆè®Šæ›´ï¼Œè·³é Git æ¨é€")
            
    except Exception as e:
        print(f"Git æ¨é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
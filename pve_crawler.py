from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from datetime import datetime
from git import Repo
import shutil
import threading
import tempfile
from pathlib import Path
import uuid
import psutil

# è‡ªå‹•æ¸…é™¤å£æ‰çš„ .wdm cache
cache_dir = os.path.join(os.path.expanduser("~"), ".wdm")
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print("å·²æ¸…é™¤ .wdm cacheï¼Œç­‰å¾…é‡æ–°ä¸‹è¼‰ä¹¾æ·¨çš„ driver")

# å®šç¾©æ‰€æœ‰å±¬æ€§çˆ¬èŸ²ä»»å‹™
POKEMON_TYPES = ["normal", "fire", "water", "electric", "grass", "ice", "fighting", 
                "poison", "ground", "flying", "psychic", "bug", "rock", "ghost", 
                "dragon", "dark", "steel", "fairy"]

# ç”Ÿæˆçˆ¬èŸ²ä»»å‹™åˆ—è¡¨
CRAWLER_TASKS = []
for i, ptype in enumerate(POKEMON_TYPES):
    CRAWLER_TASKS.append({
        "crawler_id": f"Crawler-{ptype}",
        "filename": f"{ptype}.csv",
        "url": f"https://db.pokemongohub.net/pokemon-list/best-per-type/{ptype}",
        "debug_port": 9225 + i,  # å¾9225é–‹å§‹åˆ†é…ç«¯å£
        "type": ptype
    })

# å…¨åŸŸè®Šæ•¸å’Œé–
driver_lock = threading.Lock()
driver_path = None

def get_driver_path():
    """ç·šç¨‹å®‰å…¨åœ°ç²å– ChromeDriver è·¯å¾‘"""
    global driver_path
    with driver_lock:
        if driver_path is None:
            print("æ­£åœ¨ä¸‹è¼‰å’Œå®‰è£ ChromeDriver...")
            driver_path = ChromeDriverManager().install()
            print(f"ChromeDriver å®‰è£å®Œæˆ: {driver_path}")
            
            # ç¢ºä¿æª”æ¡ˆæ¬Šé™æ­£ç¢º
            try:
                os.chmod(driver_path, 0o755)
            except Exception as e:
                print(f"è¨­å®š ChromeDriver æ¬Šé™æ™‚ç™¼ç”Ÿè­¦å‘Š: {e}")
        
        return driver_path

def create_unique_user_data_dir(crawler_id):
    """ç‚ºæ¯å€‹çˆ¬èŸ²å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„"""
    temp_dir = tempfile.gettempdir()
    unique_dir = os.path.join(temp_dir, f"chrome_profile_{crawler_id}_{uuid.uuid4().hex[:8]}")
    os.makedirs(unique_dir, exist_ok=True)
    return unique_dir

def setup_driver(crawler_id, debug_port):
    """ç‚ºæŒ‡å®šçš„çˆ¬èŸ²è¨­å®š Selenium WebDriver"""
    print(f"[{crawler_id}] æ­£åœ¨åˆå§‹åŒ– WebDriver...")
    
    # å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„
    user_data_dir = create_unique_user_data_dir(crawler_id)
    
    options = webdriver.ChromeOptions()
    
    # åŸºæœ¬é¸é …
    options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„ headless æ¨¡å¼
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    
    # ç‚ºæ¯å€‹çˆ¬èŸ²æŒ‡å®šç¨ç«‹è³‡æº
    options.add_argument(f"--user-data-dir={user_data_dir}")
    options.add_argument(f"--remote-debugging-port={debug_port}")
    
    # æ€§èƒ½å„ªåŒ–é¸é …
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    options.add_argument("--disable-ipc-flooding-protection")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    options.add_argument("--memory-pressure-off")
    options.add_argument("--max_old_space_size=4096")
    
    # ç²å– ChromeDriver è·¯å¾‘
    chrome_driver_path = get_driver_path()
    
    driver = None
    try:
        service = Service(chrome_driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(10)
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ç«¯å£: {debug_port}")
        return driver, user_data_dir
        
    except Exception as e:
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
        if driver:
            try:
                driver.quit()
            except:
                pass
        # æ¸…ç†è³‡æº
        try:
            shutil.rmtree(user_data_dir, ignore_errors=True)
        except:
            pass
        raise
    
def clean_pokemon_name(raw_name):
    """æ¸…ç†å¯¶å¯å¤¢åç¨±ï¼Œç§»é™¤æ‹¬è™Ÿå…§å®¹ä¸¦åªä¿ç•™æœ€å¾Œä¸€å€‹å–®è©"""
    if not raw_name:
        return raw_name
    
    # ç§»é™¤æ‹¬è™ŸåŠå…¶å…§å®¹ (åŒ…æ‹¬åœ“æ‹¬è™Ÿã€æ–¹æ‹¬è™Ÿç­‰)
    import re
    cleaned_name = re.sub(r'\([^)]*\)', '', raw_name)  # ç§»é™¤åœ“æ‹¬è™Ÿå…§å®¹
    cleaned_name = re.sub(r'\[[^\]]*\]', '', cleaned_name)  # ç§»é™¤æ–¹æ‹¬è™Ÿå…§å®¹
    cleaned_name = re.sub(r'\{[^}]*\}', '', cleaned_name)  # ç§»é™¤å¤§æ‹¬è™Ÿå…§å®¹
    
    # æ¸…ç†å¤šé¤˜ç©ºæ ¼
    cleaned_name = cleaned_name.strip()
    
    # å–æœ€å¾Œä¸€å€‹ç©ºæ ¼å¾Œçš„å–®è©
    if cleaned_name and ' ' in cleaned_name:
        return cleaned_name.split()[-1]
    else:
        return cleaned_name


def cleanup_crawler_resources(driver, user_data_dir, crawler_id):
    """æ¸…ç†çˆ¬èŸ²ç›¸é—œè³‡æº"""
    if driver:
        try:
            driver.quit()
            print(f"[{crawler_id}] WebDriver å·²é—œé–‰")
        except Exception as e:
            print(f"[{crawler_id}] é—œé–‰ WebDriver æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    try:
        shutil.rmtree(user_data_dir, ignore_errors=True)
        print(f"[{crawler_id}] å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„: {user_data_dir}")
    except Exception as e:
        print(f"[{crawler_id}] æ¸…ç†è‡¨æ™‚ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def kill_chrome_processes():
    """æ¸…ç†èˆ‡æœ¬ä»»å‹™ç›¸é—œçš„ Chrome é€²ç¨‹ (ç«¯å£ 9225-9243)"""
    try:
        killed_count = 0
        target_ports = set(range(9225, 9225 + len(POKEMON_TYPES)))
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if 'chrome' in proc.info['name'].lower():
                    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸ä¸­æ˜¯å¦åŒ…å«æˆ‘å€‘çš„èª¿è©¦ç«¯å£
                    cmdline = ' '.join(proc.info.get('cmdline', []))
                    for port in target_ports:
                        if f'--remote-debugging-port={port}' in cmdline:
                            proc.kill()
                            killed_count += 1
                            print(f"çµ‚æ­¢äº†ä½¿ç”¨ç«¯å£ {port} çš„ Chrome é€²ç¨‹ (PID: {proc.pid})")
                            break
            except (psutil.NoSuchProcess, psutil.AccessDenied, TypeError):
                pass
                
        if killed_count > 0:
            print(f"å·²çµ‚æ­¢ {killed_count} å€‹èˆ‡æœ¬ä»»å‹™ç›¸é—œçš„ Chrome é€²ç¨‹")
        else:
            print("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ Chrome é€²ç¨‹")
    except Exception as e:
        print(f"æ¸…ç† Chrome é€²ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def run_crawler(task):
    """é‹è¡Œå–®å€‹çˆ¬èŸ²ä»»å‹™"""
    crawler_id = task["crawler_id"]
    filename = task["filename"]
    url = task["url"] 
    debug_port = task["debug_port"]
    ptype = task["type"]
    
    max_retries = 3
    
    for attempt in range(max_retries):
        driver = None
        user_data_dir = None
        
        try:
            print(f"[{crawler_id}] é–‹å§‹çˆ¬å– {ptype} å±¬æ€§è³‡æ–™ (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            # åˆå§‹åŒ– WebDriver
            driver, user_data_dir = setup_driver(crawler_id, debug_port)
            
            # è¼‰å…¥é é¢
            print(f"[{crawler_id}] æ­£åœ¨è¼‰å…¥é é¢: {url}")
            driver.get(url)
            time.sleep(5)  # é¡å¤–ç­‰å¾…æ™‚é–“
            
            # 1ï¸âƒ£ å˜—è©¦å¤šç¨® selectorï¼Œæ‰¾åˆ°é é¢ä¸»è¦å…§å®¹
            print(f"[{crawler_id}] ç­‰å¾…é é¢å…§å®¹è¼‰å…¥...")
            try:
                # å…ˆå˜—è©¦åŸæœ¬çš„é¸æ“‡å™¨
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located(
                        (By.XPATH, '//div[contains(@class,"layout_layout__")]')
                    )
                )
                print(f"[{crawler_id}] æ‰¾åˆ° layout å…ƒç´ ")
            except:
                print(f"[{crawler_id}] æ‰¾ä¸åˆ° layoutï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨...")
                # å˜—è©¦å…¶ä»–å¯èƒ½çš„é¸æ“‡å™¨
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located(
                            (By.TAG_NAME, "main")
                        )
                    )
                    print(f"[{crawler_id}] æ‰¾åˆ° main å…ƒç´ ")
                except:
                    print(f"[{crawler_id}] å˜—è©¦ç­‰å¾… body è¼‰å…¥å®Œæˆ...")
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located(
                            (By.TAG_NAME, "body")
                        )
                    )
                    time.sleep(3)  # é¡å¤–ç­‰å¾…æ™‚é–“

            # 2ï¸âƒ£ å¦‚æœæœ‰ cookie å½ˆçª—ï¼Œå…ˆé—œæ‰
            try:
                cookie_btn = WebDriverWait(driver, 3).until(
                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(),"Accept")]'))
                )
                cookie_btn.click()
                print(f"[{crawler_id}] å·²é»æ“Š Accept cookies")
                time.sleep(1)
            except:
                print(f"[{crawler_id}] æ²’æœ‰æ‰¾åˆ° cookie å½ˆçª—")

            # 3ï¸âƒ£ ç­‰çµæœå€å¡Šè¼‰å…¥ï¼Œå˜—è©¦å¤šç¨®é¸æ“‡å™¨
            print(f"[{crawler_id}] å°‹æ‰¾çµæœå€å¡Š...")
            results = None
            
            # å˜—è©¦ä¸åŒçš„é¸æ“‡å™¨
            selectors = [
                '//div[contains(@class,"PokemonCounters_results__")]',
                '//div[contains(@class,"results")]',
                '//div[contains(@class, "pokemon")]',
                '//main//div',
                '//div[@class]'
            ]
            
            for selector in selectors:
                try:
                    results = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.XPATH, selector))
                    )
                    print(f"[{crawler_id}] æ‰¾åˆ°çµæœå€å¡Š: {selector}")
                    break
                except:
                    continue
            
            if results is None:
                print(f"[{crawler_id}] æ‰€æœ‰é¸æ“‡å™¨éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥å¾æ•´å€‹é é¢æŠ“å–...")
                results = driver.find_element(By.TAG_NAME, "body")

            # 4ï¸âƒ£ æŠ“å–å¯¶å¯å¤¢åç¨±ï¼Œå˜—è©¦å¤šç¨®æ–¹å¼
            print(f"[{crawler_id}] æŠ“å–å¯¶å¯å¤¢åç¨±...")
            names = []
            
            # æ–¹æ³•1: åŸæœ¬çš„æ–¹å¼
            try:
                name_elems = results.find_elements(By.XPATH, './/a[contains(@href,"/pokemon/")]')
                raw_names = [e.text.strip() for e in name_elems if e.text.strip()]
                # æ¸…ç†åç¨±ï¼šç§»é™¤æ‹¬è™Ÿå…§å®¹ä¸¦åªä¿ç•™æœ€å¾Œä¸€å€‹å–®è©
                names = [clean_pokemon_name(name) for name in raw_names if clean_pokemon_name(name)]
                print(f"[{crawler_id}] æ–¹æ³•1æ‰¾åˆ° {len(names)} å€‹åç¨±")
            except:
                print(f"[{crawler_id}] æ–¹æ³•1å¤±æ•—")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²’æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            if len(names) == 0:
                try:
                    name_elems = driver.find_elements(By.XPATH, '//a[contains(@href,"pokemon")]')
                    raw_names = [e.text.strip() for e in name_elems if e.text.strip()]
                    # æ¸…ç†åç¨±ï¼šç§»é™¤æ‹¬è™Ÿå…§å®¹ä¸¦åªä¿ç•™æœ€å¾Œä¸€å€‹å–®è©
                    names = [clean_pokemon_name(name) for name in raw_names if clean_pokemon_name(name)]
                    print(f"[{crawler_id}] æ–¹æ³•2æ‰¾åˆ° {len(names)} å€‹åç¨±")
                except:
                    print(f"[{crawler_id}] æ–¹æ³•2å¤±æ•—")
            
            # æ–¹æ³•3: æ›´å»£æ³›çš„æœå°‹
            if len(names) == 0:
                try:
                    # å˜—è©¦æ‰¾ä»»ä½•åŒ…å«pokemonçš„é€£çµ
                    name_elems = driver.find_elements(By.PARTIAL_LINK_TEXT, "")
                    all_links = [e.get_attribute('href') for e in name_elems if e.get_attribute('href')]
                    pokemon_links = [link for link in all_links if 'pokemon' in link.lower()]
                    print(f"[{crawler_id}] æ‰¾åˆ° {len(pokemon_links)} å€‹åŒ…å«pokemonçš„é€£çµ")
                    
                    if len(pokemon_links) > 0:
                        # å¾é€£çµä¸­æå–å¯¶å¯å¤¢åç¨±
                        for link in pokemon_links[:50]:  # é™åˆ¶50å€‹
                            try:
                                # å¾URLä¸­æå–å¯¶å¯å¤¢åç¨±
                                raw_name = link.split('/')[-1].replace('-', ' ').title()
                                # æ¸…ç†åç¨±ï¼šç§»é™¤æ‹¬è™Ÿå…§å®¹ä¸¦åªä¿ç•™æœ€å¾Œä¸€å€‹å–®è©
                                name = clean_pokemon_name(raw_name)
                                if name:
                                    names.append(name)
                            except:
                                continue
                        print(f"[{crawler_id}] å¾é€£çµæå–åˆ° {len(names)} å€‹åç¨±")
                        
                except Exception as e:
                    print(f"[{crawler_id}] æ–¹æ³•3å¤±æ•—: {e}")

            # å–å‰50å€‹çµæœ
            names = names[:50] if names else []
            
            if not names:
                print(f"[{crawler_id}] è­¦å‘Š: {ptype} å±¬æ€§æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™")
                if attempt < max_retries - 1:
                    continue
                else:
                    print(f"[{crawler_id}] {ptype} å±¬æ€§çˆ¬å–å¤±æ•—ï¼šæ‰¾ä¸åˆ°è³‡æ–™")
                    return False
            
            # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
            if not os.path.exists("pve"):
                os.makedirs("pve")
            
            # å„²å­˜æˆ CSV æª”æ¡ˆ
            data = pd.DataFrame({"rank": range(1, len(names) + 1), "name": names})
            data.to_csv(f"pve/{filename}", index=False, encoding="utf-8-sig")
            print(f"[{crawler_id}] âœ… {ptype} å±¬æ€§è³‡æ–™å·²æˆåŠŸæŠ“å–ä¸¦å„²å­˜ (å…± {len(names)} ç­†è³‡æ–™)")
            return True
            
        except Exception as e:
            print(f"[{crawler_id}] âŒ çˆ¬å– {ptype} å±¬æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"[{crawler_id}] {ptype} å±¬æ€§çˆ¬å–å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
            else:
                time.sleep(3)  # ç­‰å¾…å¾Œé‡è©¦
        finally:
            # æ¸…ç†è³‡æº
            if driver or user_data_dir:
                cleanup_crawler_resources(driver, user_data_dir, crawler_id)
    
    return False

def main():
    """ä¸»ç¨‹å¼ - å¤šå€‹çˆ¬èŸ²ä¸¦è¡ŒåŸ·è¡Œ"""
    print("=" * 80)
    print(f"å•Ÿå‹• {len(POKEMON_TYPES)} å€‹ä¸¦è¡Œçˆ¬èŸ²ï¼Œçˆ¬å–æ‰€æœ‰å¯¶å¯å¤¢å±¬æ€§çš„PVEè³‡æ–™")
    print("=" * 80)
    
    # é å…ˆæ¸…ç†å¯èƒ½æ®˜ç•™çš„ Chrome é€²ç¨‹
    print("æ¸…ç†æ®˜ç•™çš„ Chrome é€²ç¨‹...")
    kill_chrome_processes()
    
    # é å…ˆå®‰è£ ChromeDriverï¼ˆé¿å…ç«¶çˆ­ï¼‰
    print("é å…ˆæº–å‚™ ChromeDriver...")
    get_driver_path()
    
    # é¡¯ç¤ºä»»å‹™åˆ†é…
    print("\nä»»å‹™åˆ†é…:")
    for i, task in enumerate(CRAWLER_TASKS):
        print(f"  {task['crawler_id']} -> {task['type']} å±¬æ€§ (ç«¯å£: {task['debug_port']})")
        if (i + 1) % 3 == 0:  # æ¯3å€‹æ›è¡Œ
            print()
    
    print(f"\né–‹å§‹ä¸¦è¡ŒåŸ·è¡Œ {len(CRAWLER_TASKS)} å€‹çˆ¬èŸ²ä»»å‹™...")
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œï¼Œæ¯å€‹ç·šç¨‹è™•ç†ä¸€å€‹ä»»å‹™
    # å›ºå®šä½¿ç”¨6å€‹åŒæ™‚åŸ·è¡Œçš„ç·šç¨‹
    max_workers = 6
    
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="PVE-Crawler") as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(run_crawler, task): task["crawler_id"] 
            for task in CRAWLER_TASKS
        }
        
        # ç­‰å¾…å®Œæˆä¸¦è™•ç†çµæœ
        results = {}
        completed_count = 0
        
        for future in as_completed(futures):
            crawler_id = futures[future]
            try:
                success = future.result()
                results[crawler_id] = success
                completed_count += 1
                
                if success:
                    print(f"ğŸ‰ {crawler_id} ä»»å‹™å®Œæˆ ({completed_count}/{len(CRAWLER_TASKS)})")
                else:
                    print(f"âš ï¸  {crawler_id} ä»»å‹™å¤±æ•— ({completed_count}/{len(CRAWLER_TASKS)})")
            except Exception as e:
                print(f"âŒ {crawler_id} åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[crawler_id] = False
                completed_count += 1
    
    # çµ±è¨ˆçµæœ
    success_count = sum(results.values())
    total_count = len(CRAWLER_TASKS)
    
    print("\n" + "=" * 80)
    print("çˆ¬å–çµæœç¸½è¦½:")
    
    # æŒ‰å±¬æ€§åˆ†çµ„é¡¯ç¤ºçµæœ
    for i, (crawler_id, success) in enumerate(results.items()):
        ptype = crawler_id.replace("Crawler-", "")
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        print(f"  {ptype.ljust(10)}: {status}")
        if (i + 1) % 6 == 0:  # æ¯6å€‹æ›è¡Œ
            print()
    
    print(f"\nç¸½è¨ˆ: {success_count}/{total_count} å€‹å±¬æ€§æˆåŠŸå®Œæˆ")
    print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    print("=" * 80)
    
    # æœ€çµ‚æ¸…ç†
    print("åŸ·è¡Œæœ€çµ‚æ¸…ç†...")
    time.sleep(2)
    kill_chrome_processes()
    
    # åˆä½µæ‰€æœ‰CSVæª”æ¡ˆ
    merge_success = merge_csv_files()
    
    if merge_success:
        print("\næ‰€æœ‰ä»»å‹™å®Œæˆï¼åˆä½µæª”æ¡ˆå·²å„²å­˜åˆ° data/pve.csv")
    else:
        print("\nçˆ¬å–å®Œæˆï¼Œä½†åˆä½µæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤")
    
    # å¯é¸ï¼šæ¨é€åˆ° GitHub
    # push_to_github()

def merge_csv_files():
    """åˆä½µæ‰€æœ‰å±¬æ€§çš„CSVæª”æ¡ˆæˆä¸€å€‹ç¸½æª”æ¡ˆ"""
    print("\né–‹å§‹åˆä½µæ‰€æœ‰CSVæª”æ¡ˆ...")
    
    all_data = []
    successful_files = []
    
    # ç¢ºä¿ data è³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists("data"):
        os.makedirs("data")
    
    for ptype in POKEMON_TYPES:
        csv_path = f"pve/{ptype}.csv"
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path, encoding="utf-8-sig")
                # åŠ å…¥å±¬æ€§æ¬„ä½
                df['type'] = ptype
                # é‡æ–°æ’åºæ¬„ä½: type, rank, name
                df = df[['type', 'rank', 'name']]
                all_data.append(df)
                successful_files.append(ptype)
                print(f"âœ… å·²è®€å– {ptype} å±¬æ€§è³‡æ–™ ({len(df)} ç­†)")
            except Exception as e:
                print(f"âŒ è®€å– {ptype}.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        else:
            print(f"âš ï¸  æ‰¾ä¸åˆ° {ptype}.csv æª”æ¡ˆ")
    
    if all_data:
        # åˆä½µæ‰€æœ‰è³‡æ–™
        merged_df = pd.concat(all_data, ignore_index=True)
        
        # å„²å­˜åˆä½µçµæœ
        output_path = "data/pve.csv"
        merged_df.to_csv(output_path, index=False, encoding="utf-8-sig")
        
        print(f"\nğŸ‰ åˆä½µå®Œæˆï¼")
        print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {output_path}")
        print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(merged_df)} ç­†è³‡æ–™")
        print(f"ğŸ“‹ æˆåŠŸåˆä½µçš„å±¬æ€§: {len(successful_files)}/{len(POKEMON_TYPES)}")
        
        # é¡¯ç¤ºæ¯å€‹å±¬æ€§çš„è³‡æ–™ç­†æ•¸çµ±è¨ˆ
        print(f"\nå„å±¬æ€§è³‡æ–™çµ±è¨ˆ:")
        type_counts = merged_df['type'].value_counts().sort_index()
        for ptype, count in type_counts.items():
            print(f"  {ptype.ljust(10)}: {count} ç­†")
            
        return True
    else:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆä½µçš„è³‡æ–™")
        return False
    
    # """å°‡æ›´æ–°çš„æª”æ¡ˆæ¨é€åˆ° GitHub"""
    # try:
    #     repo = Repo(os.getcwd())
        
    #     # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
    #     if repo.is_dirty() or repo.untracked_files:
    #         # æ·»åŠ åˆä½µå¾Œçš„PVEè³‡æ–™æª”æ¡ˆ
    #         repo.git.add(['data/pve.csv'])
    #         repo.index.commit(f"è‡ªå‹•æ›´æ–° PVE åˆä½µè³‡æ–™ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    #         origin = repo.remote(name='origin')
    #         origin.push()
    #         print("å·²æ¨é€æ›´æ–°åˆ° GitHub")
    #     else:
    #         print("æ²’æœ‰æª”æ¡ˆè®Šæ›´ï¼Œè·³é Git æ¨é€")
            
    # except Exception as e:
    #     print(f"Git æ¨é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from datetime import datetime
from git import Repo
import shutil
import threading
import tempfile
from pathlib import Path
import uuid
import psutil
import signal
import sys

# è‡ªå‹•æ¸…é™¤å£æ‰çš„ .wdm cache
cache_dir = os.path.join(os.path.expanduser("~"), ".wdm")
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print("å·²æ¸…é™¤ .wdm cacheï¼Œç­‰å¾…é‡æ–°ä¸‹è¼‰ä¹¾æ·¨çš„ driver")

# å®šç¾©3å€‹çˆ¬èŸ²ä»»å‹™
CRAWLER_TASKS = [
    {
        "crawler_id": "Crawler-1500",
        "filename": "pvpoke_1500.csv",
        "url": "https://pvpoketw.com/rankings/all/1500/overall/",
        "debug_port": 9222
    },
    {
        "crawler_id": "Crawler-2500", 
        "filename": "pvpoke_2500.csv",
        "url": "https://pvpoketw.com/rankings/all/2500/overall/",
        "debug_port": 9223
    },
    {
        "crawler_id": "Crawler-10000",
        "filename": "pvpoke_10000.csv", 
        "url": "https://pvpoketw.com/rankings/all/10000/overall/",
        "debug_port": 9224
    }
]

# å…¨åŸŸè®Šæ•¸å’Œé–
driver_lock = threading.Lock()
driver_path = None

def signal_handler(sig, frame):
    """è™•ç†ä¸­æ–·ä¿¡è™Ÿï¼Œç¢ºä¿æ¸…ç†è³‡æº"""
    print("\næ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨æ¸…ç†è³‡æº...")
    kill_chrome_processes()
    sys.exit(0)

def get_driver_path():
    """ç·šç¨‹å®‰å…¨åœ°ç²å– ChromeDriver è·¯å¾‘"""
    global driver_path
    with driver_lock:
        if driver_path is None:
            print("æ­£åœ¨ä¸‹è¼‰å’Œå®‰è£ ChromeDriver...")
            driver_path = ChromeDriverManager().install()
            print(f"ChromeDriver å®‰è£å®Œæˆ: {driver_path}")
            
            # ç¢ºä¿æª”æ¡ˆæ¬Šé™æ­£ç¢º
            try:
                os.chmod(driver_path, 0o755)
            except Exception as e:
                print(f"è¨­å®š ChromeDriver æ¬Šé™æ™‚ç™¼ç”Ÿè­¦å‘Š: {e}")
        
        return driver_path

def create_unique_user_data_dir(crawler_id):
    """ç‚ºæ¯å€‹çˆ¬èŸ²å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„"""
    temp_dir = tempfile.gettempdir()
    unique_dir = os.path.join(temp_dir, f"chrome_profile_{crawler_id}_{uuid.uuid4().hex[:8]}")
    os.makedirs(unique_dir, exist_ok=True)
    return unique_dir

def setup_driver(crawler_id, debug_port):
    """ç‚ºæŒ‡å®šçš„çˆ¬èŸ²è¨­å®š Selenium WebDriver"""
    print(f"[{crawler_id}] æ­£åœ¨åˆå§‹åŒ– WebDriver...")
    
    # å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„
    user_data_dir = create_unique_user_data_dir(crawler_id)
    
    options = webdriver.ChromeOptions()
    
    # åŸºæœ¬é¸é …
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    # ç‚ºæ¯å€‹çˆ¬èŸ²æŒ‡å®šç¨ç«‹è³‡æº
    options.add_argument(f"--user-data-dir={user_data_dir}")
    options.add_argument(f"--remote-debugging-port={debug_port}")
    
    # æ€§èƒ½å„ªåŒ–é¸é … - ä¿ç•™ JavaScriptï¼ˆç¶²ç«™éœ€è¦ï¼‰
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    # ä¸è¦ç¦ç”¨åœ–ç‰‡ï¼Œå¯èƒ½å½±éŸ¿é é¢è¼‰å…¥åˆ¤æ–·
    # options.add_argument("--disable-images")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    options.add_argument("--disable-ipc-flooding-protection")
    
    # è¦–çª—å¤§å° - ç¢ºä¿é é¢æ­£å¸¸æ¸²æŸ“
    options.add_argument("--window-size=1920,1080")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    options.add_argument("--memory-pressure-off")
    options.add_argument("--max_old_space_size=2048")
    
    # ç©©å®šæ€§é¸é …
    options.add_argument("--disable-background-timer-throttling")
    options.add_argument("--disable-backgrounding-occluded-windows")
    options.add_argument("--disable-renderer-backgrounding")
    options.add_argument("--disable-features=TranslateUI")
    options.add_argument("--disable-default-apps")
    options.add_argument("--no-first-run")
    options.add_argument("--disable-sync")
    
    # User Agent - æ¨¡æ“¬æ­£å¸¸ç€è¦½å™¨
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # ç²å– ChromeDriver è·¯å¾‘
    chrome_driver_path = get_driver_path()
    
    driver = None
    try:
        service = Service(chrome_driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        
        # å¢åŠ è¶…æ™‚æ™‚é–“
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(10)
        
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ç«¯å£: {debug_port}")
        return driver, user_data_dir
        
    except Exception as e:
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
        if driver:
            try:
                driver.quit()
            except:
                pass
        # æ¸…ç†è³‡æº
        try:
            shutil.rmtree(user_data_dir, ignore_errors=True)
        except:
            pass
        raise

def cleanup_crawler_resources(driver, user_data_dir, crawler_id):
    """æ¸…ç†çˆ¬èŸ²ç›¸é—œè³‡æº"""
    if driver:
        try:
            driver.quit()
            print(f"[{crawler_id}] WebDriver å·²é—œé–‰")
        except Exception as e:
            print(f"[{crawler_id}] é—œé–‰ WebDriver æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    try:
        shutil.rmtree(user_data_dir, ignore_errors=True)
        print(f"[{crawler_id}] å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„: {user_data_dir}")
    except Exception as e:
        print(f"[{crawler_id}] æ¸…ç†è‡¨æ™‚ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def kill_chrome_processes():
    """åªæ¸…ç†çˆ¬èŸ²ç›¸é—œçš„ Chrome é€²ç¨‹ï¼Œä¿ç•™æ­£å¸¸ä½¿ç”¨çš„ Chrome"""
    try:
        killed_count = 0
        crawler_ports = [9222, 9223, 9224]
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if 'chrome' in proc.info['name'].lower():
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    
                    should_kill = False
                    
                    for port in crawler_ports:
                        if f'--remote-debugging-port={port}' in cmdline:
                            should_kill = True
                            break
                    
                    if '--headless' in cmdline:
                        should_kill = True
                    
                    if 'chrome_profile_Crawler' in cmdline:
                        should_kill = True
                    
                    if should_kill:
                        proc.kill()
                        killed_count += 1
                        print(f"å·²çµ‚æ­¢çˆ¬èŸ² Chrome é€²ç¨‹ PID: {proc.info['pid']}")
                        
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                pass
        
        if killed_count > 0:
            print(f"å…±çµ‚æ­¢ {killed_count} å€‹çˆ¬èŸ²ç›¸é—œçš„ Chrome é€²ç¨‹")
        else:
            print("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„çˆ¬èŸ² Chrome é€²ç¨‹")
            
    except Exception as e:
        print(f"æ¸…ç†çˆ¬èŸ² Chrome é€²ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def scroll_and_wait(driver, crawler_id):
    """æ»¾å‹•é é¢ä¸¦ç­‰å¾…å‹•æ…‹å…§å®¹è¼‰å…¥"""
    print(f"[{crawler_id}] æ»¾å‹•é é¢ä»¥è§¸ç™¼å‹•æ…‹è¼‰å…¥...")
    
    try:
        # æ»¾å‹•åˆ°é é¢åº•éƒ¨å†å›åˆ°é ‚éƒ¨ï¼Œè§¸ç™¼æ‰€æœ‰å…§å®¹è¼‰å…¥
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(1)
        
        # å†æ¬¡æ»¾å‹•åˆ°ä¸­é–“ä½ç½®
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
        time.sleep(1)
        
    except Exception as e:
        print(f"[{crawler_id}] æ»¾å‹•é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def wait_for_pokemon_data(driver, crawler_id, max_wait=30):
    """ç­‰å¾… Pokemon è³‡æ–™è¼‰å…¥å®Œæˆ - æ”¹é€²ç‰ˆ"""
    print(f"[{crawler_id}] ç­‰å¾…é é¢å…ƒç´ è¼‰å…¥...")
    
    # é¦–å…ˆç­‰å¾…åŸºæœ¬é é¢è¼‰å…¥
    time.sleep(5)
    
    # æ»¾å‹•é é¢è§¸ç™¼å‹•æ…‹è¼‰å…¥
    scroll_and_wait(driver, crawler_id)
    
    # ç­‰å¾…ä¸¦æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„è³‡æ–™
    for i in range(max_wait):
        try:
            elements = driver.find_elements(By.CLASS_NAME, "name")
            
            # æª¢æŸ¥æœ‰å¤šå°‘å…ƒç´ æœ‰å¯¦éš›æ–‡å­—å…§å®¹
            elements_with_text = [e for e in elements if e.text.strip()]
            
            print(f"[{crawler_id}] ç¬¬ {i+1} ç§’: æ‰¾åˆ° {len(elements)} å€‹ name å…ƒç´ , {len(elements_with_text)} å€‹æœ‰æ–‡å­—")
            
            # å¦‚æœæ‰¾åˆ°è¶…é 50 å€‹æœ‰æ–‡å­—çš„å…ƒç´ ï¼Œèªç‚ºè¼‰å…¥å®Œæˆ
            if len(elements_with_text) > 50:
                print(f"[{crawler_id}] âœ“ é é¢è¼‰å…¥å®Œæˆï¼Œæ‰¾åˆ° {len(elements_with_text)} å€‹æœ‰æ•ˆå…ƒç´ ")
                return True
            
            # æ¯ 5 ç§’æ»¾å‹•ä¸€æ¬¡é é¢
            if i > 0 and i % 5 == 0:
                scroll_and_wait(driver, crawler_id)
            
            time.sleep(1)
            
        except Exception as e:
            print(f"[{crawler_id}] ç­‰å¾…æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            time.sleep(1)
    
    # æœ€å¾Œå†æª¢æŸ¥ä¸€æ¬¡
    try:
        elements = driver.find_elements(By.CLASS_NAME, "name")
        elements_with_text = [e for e in elements if e.text.strip()]
        if len(elements_with_text) > 20:
            print(f"[{crawler_id}] ç­‰å¾…è¶…æ™‚ï¼Œä½†æ‰¾åˆ° {len(elements_with_text)} å€‹å…ƒç´ ï¼Œç¹¼çºŒè™•ç†")
            return True
    except:
        pass
    
    print(f"[{crawler_id}] ç­‰å¾…è¶…æ™‚ï¼Œè³‡æ–™å¯èƒ½æœªå®Œå…¨è¼‰å…¥")
    return False

def extract_pokemon_data(driver, crawler_id):
    """æå– Pokemon è³‡æ–™ - æ”¹é€²ç‰ˆ"""
    try:
        # å†æ¬¡ç¢ºä¿é é¢å®Œå…¨è¼‰å…¥
        time.sleep(2)
        
        name_elements = driver.find_elements(By.CLASS_NAME, "name")
        print(f"[{crawler_id}] æ‰¾åˆ° {len(name_elements)} å€‹ name å…ƒç´ ")

        if not name_elements:
            return [], []

        clean_names = []
        xl_list = []
        
        skipped_empty = 0
        skipped_invalid = 0

        for e in name_elements:
            try:
                text = e.text.strip()
                html = e.get_attribute("innerHTML")

                if not text:
                    skipped_empty += 1
                    continue
                
                # éæ¿¾æ‰å°èˆª/é¸å–®é …ç›®ï¼ˆé€šå¸¸æ˜¯å–®å­—ä¸”ä¸å«æ•¸å­—ï¼‰
                # Pokemon åç¨±é€šå¸¸è¼ƒé•·æˆ–åŒ…å«ç‰¹å®šæ¨¡å¼
                if len(text) < 2:
                    skipped_invalid += 1
                    continue

                # æ¸…ç†åå­—ï¼šç§»é™¤ XL æ¨™è¨˜ä¸¦å–ç¬¬ä¸€å€‹è©
                clean_name = text.replace("XL", "").strip().split(" ")[0]
                
                if not clean_name or len(clean_name) < 2:
                    skipped_invalid += 1
                    continue
                
                clean_names.append(clean_name)

                # XL åˆ¤æ–·
                xl_list.append(1 if "xl-info-icon" in html else 0)
                
            except Exception as e_inner:
                print(f"[{crawler_id}] è™•ç†å–®å€‹å…ƒç´ æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_inner}")
                continue

        print(f"[{crawler_id}] æˆåŠŸæå– {len(clean_names)} å€‹å¯¶å¯å¤¢è³‡æ–™")
        print(f"[{crawler_id}] (è·³é {skipped_empty} å€‹ç©ºå…ƒç´ , {skipped_invalid} å€‹ç„¡æ•ˆå…ƒç´ )")
        
        return clean_names, xl_list

    except Exception as e:
        print(f"[{crawler_id}] æå–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return [], []


def run_crawler(task):
    """é‹è¡Œå–®å€‹çˆ¬èŸ²ä»»å‹™"""
    crawler_id = task["crawler_id"]
    filename = task["filename"]
    url = task["url"] 
    debug_port = task["debug_port"]
    
    max_retries = 3
    
    for attempt in range(max_retries):
        driver = None
        user_data_dir = None
        
        try:
            print(f"[{crawler_id}] é–‹å§‹çˆ¬å– {filename} (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            # åˆå§‹åŒ– WebDriver
            driver, user_data_dir = setup_driver(crawler_id, debug_port)
            
            # è¼‰å…¥é é¢
            print(f"[{crawler_id}] æ­£åœ¨è¼‰å…¥é é¢: {url}")
            driver.get(url)
            
            # ä½¿ç”¨æ”¹é€²çš„ç­‰å¾…ç­–ç•¥
            wait_for_pokemon_data(driver, crawler_id)
            
            # æå–è³‡æ–™
            names_list, xl_list = extract_pokemon_data(driver, crawler_id)
            
            if not names_list:
                print(f"[{crawler_id}] è­¦å‘Š: {filename} æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™")
                if attempt < max_retries - 1:
                    print(f"[{crawler_id}] æº–å‚™é‡è©¦...")
                    # å¢åŠ é‡è©¦é–“éš”
                    time.sleep(5)
                    continue
                else:
                    print(f"[{crawler_id}] {filename} çˆ¬å–å¤±æ•—ï¼šæ‰¾ä¸åˆ°è³‡æ–™")
                    return False
            
            # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
            if not os.path.exists("data"):
                os.makedirs("data")
            
            # å„²å­˜æˆ CSV æª”æ¡ˆ
            data = pd.DataFrame({"Pokemon": names_list, "XL": xl_list})
            data.to_csv(f"data/{filename}", index=False, encoding="utf-8-sig")
            
            # çµ±è¨ˆçµæœ
            xl_count = sum(xl_list)
            total_count = len(names_list)
            print(f"[{crawler_id}] âœ… {filename} è³‡æ–™å·²æˆåŠŸæŠ“å–ä¸¦å„²å­˜")
            print(f"[{crawler_id}] ç¸½è¨ˆ {total_count} éš»å¯¶å¯å¤¢ï¼Œå…¶ä¸­ {xl_count} éš»éœ€è¦XLç³–")
            return True
            
        except Exception as e:
            print(f"[{crawler_id}] âŒ çˆ¬å– {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"[{crawler_id}] {filename} çˆ¬å–å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
            else:
                time.sleep(5)  # å¢åŠ é‡è©¦é–“éš”
        finally:
            # æ¸…ç†è³‡æº
            if driver or user_data_dir:
                cleanup_crawler_resources(driver, user_data_dir, crawler_id)
    
    return False

def main():
    """ä¸»ç¨‹å¼ - 3å€‹çˆ¬èŸ²ä¸¦è¡ŒåŸ·è¡Œ"""
    # è¨­å®šä¿¡è™Ÿè™•ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("=" * 60)
    print("å•Ÿå‹• 3 å€‹ä¸¦è¡Œçˆ¬èŸ²ï¼Œæ¯å€‹è™•ç†ä¸åŒçš„ä»»å‹™")
    print("=" * 60)
    
    # é å…ˆæ¸…ç†å¯èƒ½æ®˜ç•™çš„ Chrome é€²ç¨‹
    print("æ¸…ç†æ®˜ç•™çš„ Chrome é€²ç¨‹...")
    kill_chrome_processes()
    
    # é å…ˆå®‰è£ ChromeDriverï¼ˆé¿å…ç«¶çˆ­ï¼‰
    print("é å…ˆæº–å‚™ ChromeDriver...")
    get_driver_path()
    
    # é¡¯ç¤ºä»»å‹™åˆ†é…
    print("\nä»»å‹™åˆ†é…:")
    for task in CRAWLER_TASKS:
        print(f"  {task['crawler_id']} -> {task['filename']} (ç«¯å£: {task['debug_port']})")
    
    print(f"\né–‹å§‹ä¸¦è¡ŒåŸ·è¡Œ {len(CRAWLER_TASKS)} å€‹çˆ¬èŸ²ä»»å‹™...")
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œ
    with ThreadPoolExecutor(max_workers=3, thread_name_prefix="Crawler") as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(run_crawler, task): task["crawler_id"] 
            for task in CRAWLER_TASKS
        }
        
        # ç­‰å¾…å®Œæˆä¸¦è™•ç†çµæœ
        results = {}
        for future in as_completed(futures):
            crawler_id = futures[future]
            try:
                success = future.result()
                results[crawler_id] = success
                if success:
                    print(f"ğŸ‰ {crawler_id} ä»»å‹™å®Œæˆ")
                else:
                    print(f"âš ï¸  {crawler_id} ä»»å‹™å¤±æ•—")
            except Exception as e:
                print(f"âŒ {crawler_id} åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[crawler_id] = False
    
    # çµ±è¨ˆçµæœ
    success_count = sum(results.values())
    total_count = len(CRAWLER_TASKS)
    
    print("\n" + "=" * 60)
    print("çˆ¬å–çµæœç¸½è¦½:")
    for crawler_id, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        print(f"  {crawler_id}: {status}")
    
    print(f"\nç¸½è¨ˆ: {success_count}/{total_count} å€‹ä»»å‹™æˆåŠŸå®Œæˆ")
    print("=" * 60)
    
    # æœ€çµ‚æ¸…ç†
    print("åŸ·è¡Œæœ€çµ‚æ¸…ç†...")
    time.sleep(2)
    kill_chrome_processes()
    
    # å¦‚æœéœ€è¦æ¨é€åˆ° GitHubï¼Œå–æ¶ˆè¨»è§£ä¸‹é¢é€™è¡Œ
    # push_to_github()

def push_to_github():
    """å°‡æ›´æ–°çš„æª”æ¡ˆæ¨é€åˆ° GitHub"""
    try:
        repo = Repo(os.getcwd())
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
        if repo.is_dirty() or repo.untracked_files:
            repo.git.add(['data/pvpoke_1500.csv', 'data/pvpoke_2500.csv', 'data/pvpoke_10000.csv'])
            repo.index.commit(f"è‡ªå‹•æ›´æ–° PvP è³‡æ–™ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            origin = repo.remote(name='origin')
            origin.push()
            print("å·²æ¨é€æ›´æ–°åˆ° GitHub")
        else:
            print("æ²’æœ‰æª”æ¡ˆè®Šæ›´ï¼Œè·³é Git æ¨é€")
            
    except Exception as e:
        print(f"Git æ¨é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
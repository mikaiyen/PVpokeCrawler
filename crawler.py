from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from datetime import datetime
from git import Repo
import shutil
import threading
import tempfile
from pathlib import Path
import uuid
import psutil

# è‡ªå‹•æ¸…é™¤å£æ‰çš„ .wdm cache
cache_dir = os.path.join(os.path.expanduser("~"), ".wdm")
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print("å·²æ¸…é™¤ .wdm cacheï¼Œç­‰å¾…é‡æ–°ä¸‹è¼‰ä¹¾æ·¨çš„ driver")

# å®šç¾©3å€‹çˆ¬èŸ²ä»»å‹™
CRAWLER_TASKS = [
    {
        "crawler_id": "Crawler-1500",
        "filename": "pvpoke_1500.csv",
        "url": "https://pvpoketw.com/rankings/all/1500/overall/",
        "debug_port": 9222
    },
    {
        "crawler_id": "Crawler-2500", 
        "filename": "pvpoke_2500.csv",
        "url": "https://pvpoketw.com/rankings/all/2500/overall/",
        "debug_port": 9223
    },
    {
        "crawler_id": "Crawler-10000",
        "filename": "pvpoke_10000.csv", 
        "url": "https://pvpoketw.com/rankings/all/10000/overall/",
        "debug_port": 9224
    }
]

# å…¨åŸŸè®Šæ•¸å’Œé–
driver_lock = threading.Lock()
driver_path = None

def get_driver_path():
    """ç·šç¨‹å®‰å…¨åœ°ç²å– ChromeDriver è·¯å¾‘"""
    global driver_path
    with driver_lock:
        if driver_path is None:
            print("æ­£åœ¨ä¸‹è¼‰å’Œå®‰è£ ChromeDriver...")
            driver_path = ChromeDriverManager().install()
            print(f"ChromeDriver å®‰è£å®Œæˆ: {driver_path}")
            
            # ç¢ºä¿æª”æ¡ˆæ¬Šé™æ­£ç¢º
            try:
                os.chmod(driver_path, 0o755)
            except Exception as e:
                print(f"è¨­å®š ChromeDriver æ¬Šé™æ™‚ç™¼ç”Ÿè­¦å‘Š: {e}")
        
        return driver_path

def create_unique_user_data_dir(crawler_id):
    """ç‚ºæ¯å€‹çˆ¬èŸ²å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„"""
    temp_dir = tempfile.gettempdir()
    unique_dir = os.path.join(temp_dir, f"chrome_profile_{crawler_id}_{uuid.uuid4().hex[:8]}")
    os.makedirs(unique_dir, exist_ok=True)
    return unique_dir

def setup_driver(crawler_id, debug_port):
    """ç‚ºæŒ‡å®šçš„çˆ¬èŸ²è¨­å®š Selenium WebDriver"""
    print(f"[{crawler_id}] æ­£åœ¨åˆå§‹åŒ– WebDriver...")
    
    # å‰µå»ºç¨ç«‹çš„ç”¨æˆ¶è³‡æ–™ç›®éŒ„
    user_data_dir = create_unique_user_data_dir(crawler_id)
    
    options = webdriver.ChromeOptions()
    
    # åŸºæœ¬é¸é …
    options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„ headless æ¨¡å¼
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    # ç‚ºæ¯å€‹çˆ¬èŸ²æŒ‡å®šç¨ç«‹è³‡æº
    options.add_argument(f"--user-data-dir={user_data_dir}")
    options.add_argument(f"--remote-debugging-port={debug_port}")
    
    # æ€§èƒ½å„ªåŒ–é¸é …
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    options.add_argument("--disable-images")
    options.add_argument("--disable-javascript")
    options.add_argument("--disable-css")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    options.add_argument("--disable-ipc-flooding-protection")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    options.add_argument("--memory-pressure-off")
    options.add_argument("--max_old_space_size=4096")
    
    # ç²å– ChromeDriver è·¯å¾‘
    chrome_driver_path = get_driver_path()
    
    driver = None
    try:
        service = Service(chrome_driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(10)
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ç«¯å£: {debug_port}")
        return driver, user_data_dir
        
    except Exception as e:
        print(f"[{crawler_id}] WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
        if driver:
            try:
                driver.quit()
            except:
                pass
        # æ¸…ç†è³‡æº
        try:
            shutil.rmtree(user_data_dir, ignore_errors=True)
        except:
            pass
        raise

def cleanup_crawler_resources(driver, user_data_dir, crawler_id):
    """æ¸…ç†çˆ¬èŸ²ç›¸é—œè³‡æº"""
    if driver:
        try:
            driver.quit()
            print(f"[{crawler_id}] WebDriver å·²é—œé–‰")
        except Exception as e:
            print(f"[{crawler_id}] é—œé–‰ WebDriver æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
    try:
        shutil.rmtree(user_data_dir, ignore_errors=True)
        print(f"[{crawler_id}] å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„: {user_data_dir}")
    except Exception as e:
        print(f"[{crawler_id}] æ¸…ç†è‡¨æ™‚ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def kill_chrome_processes():
    """åªæ¸…ç†çˆ¬èŸ²ç›¸é—œçš„ Chrome é€²ç¨‹ï¼Œä¿ç•™æ­£å¸¸ä½¿ç”¨çš„ Chrome"""
    try:
        killed_count = 0
        crawler_ports = [9222, 9223, 9224]  # çˆ¬èŸ²ä½¿ç”¨çš„é™¤éŒ¯ç«¯å£
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if 'chrome' in proc.info['name'].lower():
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    
                    # åªæ®ºæ­»åŒ…å«çˆ¬èŸ²ç›¸é—œåƒæ•¸çš„ Chrome é€²ç¨‹
                    should_kill = False
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«çˆ¬èŸ²ä½¿ç”¨çš„é™¤éŒ¯ç«¯å£
                    for port in crawler_ports:
                        if f'--remote-debugging-port={port}' in cmdline:
                            should_kill = True
                            break
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å« headless åƒæ•¸ï¼ˆçˆ¬èŸ²ç‰¹æœ‰ï¼‰
                    if '--headless' in cmdline:
                        should_kill = True
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«çˆ¬èŸ²çš„è‡¨æ™‚ç›®éŒ„
                    if 'chrome_profile_Crawler' in cmdline:
                        should_kill = True
                    
                    if should_kill:
                        proc.kill()
                        killed_count += 1
                        print(f"å·²çµ‚æ­¢çˆ¬èŸ² Chrome é€²ç¨‹ PID: {proc.info['pid']}")
                        
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                pass
        
        if killed_count > 0:
            print(f"å…±çµ‚æ­¢ {killed_count} å€‹çˆ¬èŸ²ç›¸é—œçš„ Chrome é€²ç¨‹")
        else:
            print("æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„çˆ¬èŸ² Chrome é€²ç¨‹")
            
    except Exception as e:
        print(f"æ¸…ç†çˆ¬èŸ² Chrome é€²ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def run_crawler(task):
    """é‹è¡Œå–®å€‹çˆ¬èŸ²ä»»å‹™"""
    crawler_id = task["crawler_id"]
    filename = task["filename"]
    url = task["url"] 
    debug_port = task["debug_port"]
    
    max_retries = 3
    
    for attempt in range(max_retries):
        driver = None
        user_data_dir = None
        
        try:
            print(f"[{crawler_id}] é–‹å§‹çˆ¬å– {filename} (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            # åˆå§‹åŒ– WebDriver
            driver, user_data_dir = setup_driver(crawler_id, debug_port)
            
            # è¼‰å…¥é é¢
            print(f"[{crawler_id}] æ­£åœ¨è¼‰å…¥é é¢: {url}")
            driver.get(url)
            
            # ç­‰å¾…é é¢è¼‰å…¥
            time.sleep(5)
            
            # æŸ¥æ‰¾å…ƒç´ 
            names = driver.find_elements(By.CLASS_NAME, "name")
            
            if not names:
                print(f"[{crawler_id}] è­¦å‘Š: {filename} æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™")
                if attempt < max_retries - 1:
                    continue
                else:
                    print(f"[{crawler_id}] {filename} çˆ¬å–å¤±æ•—ï¼šæ‰¾ä¸åˆ°è³‡æ–™")
                    return False
            
            # è™•ç†è³‡æ–™
            names_list = [n.text.replace("XL", "").split(" ")[0] for n in names if n.text.strip()]
            xl_list = [1 if "XL" in n.text else 0 for n in names if n.text.strip()]
            
            if not names_list:
                print(f"[{crawler_id}] è­¦å‘Š: {filename} è™•ç†å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™")
                if attempt < max_retries - 1:
                    continue
            
            # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
            if not os.path.exists("data"):
                os.makedirs("data")
            
            # å„²å­˜æˆ CSV æª”æ¡ˆ
            data = pd.DataFrame({"Pokemon": names_list, "XL": xl_list})
            data.to_csv(f"data/{filename}", index=False, encoding="utf-8-sig")
            print(f"[{crawler_id}] âœ… {filename} è³‡æ–™å·²æˆåŠŸæŠ“å–ä¸¦å„²å­˜ (å…± {len(names_list)} ç­†è³‡æ–™)")
            return True
            
        except Exception as e:
            print(f"[{crawler_id}] âŒ çˆ¬å– {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"[{crawler_id}] {filename} çˆ¬å–å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
            else:
                time.sleep(3)  # ç­‰å¾…å¾Œé‡è©¦
        finally:
            # æ¸…ç†è³‡æº
            if driver or user_data_dir:
                cleanup_crawler_resources(driver, user_data_dir, crawler_id)
    
    return False

def main():
    """ä¸»ç¨‹å¼ - 3å€‹çˆ¬èŸ²ä¸¦è¡ŒåŸ·è¡Œ"""
    print("=" * 60)
    print("å•Ÿå‹• 3 å€‹ä¸¦è¡Œçˆ¬èŸ²ï¼Œæ¯å€‹è™•ç†ä¸åŒçš„ä»»å‹™")
    print("=" * 60)
    
    # é å…ˆæ¸…ç†å¯èƒ½æ®˜ç•™çš„ Chrome é€²ç¨‹
    print("æ¸…ç†æ®˜ç•™çš„ Chrome é€²ç¨‹...")
    kill_chrome_processes()
    
    # é å…ˆå®‰è£ ChromeDriverï¼ˆé¿å…ç«¶çˆ­ï¼‰
    print("é å…ˆæº–å‚™ ChromeDriver...")
    get_driver_path()
    
    # é¡¯ç¤ºä»»å‹™åˆ†é…
    print("\nä»»å‹™åˆ†é…:")
    for task in CRAWLER_TASKS:
        print(f"  {task['crawler_id']} -> {task['filename']} (ç«¯å£: {task['debug_port']})")
    
    print(f"\né–‹å§‹ä¸¦è¡ŒåŸ·è¡Œ {len(CRAWLER_TASKS)} å€‹çˆ¬èŸ²ä»»å‹™...")
    
    # ä½¿ç”¨ç·šç¨‹æ± åŸ·è¡Œï¼Œæ¯å€‹ç·šç¨‹è™•ç†ä¸€å€‹ä»»å‹™
    with ThreadPoolExecutor(max_workers=3, thread_name_prefix="Crawler") as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(run_crawler, task): task["crawler_id"] 
            for task in CRAWLER_TASKS
        }
        
        # ç­‰å¾…å®Œæˆä¸¦è™•ç†çµæœ
        results = {}
        for future in as_completed(futures):
            crawler_id = futures[future]
            try:
                success = future.result()
                results[crawler_id] = success
                if success:
                    print(f"ğŸ‰ {crawler_id} ä»»å‹™å®Œæˆ")
                else:
                    print(f"âš ï¸  {crawler_id} ä»»å‹™å¤±æ•—")
            except Exception as e:
                print(f"âŒ {crawler_id} åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[crawler_id] = False
    
    # çµ±è¨ˆçµæœ
    success_count = sum(results.values())
    total_count = len(CRAWLER_TASKS)
    
    print("\n" + "=" * 60)
    print("çˆ¬å–çµæœç¸½è¦½:")
    for crawler_id, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        print(f"  {crawler_id}: {status}")
    
    print(f"\nç¸½è¨ˆ: {success_count}/{total_count} å€‹ä»»å‹™æˆåŠŸå®Œæˆ")
    print("=" * 60)
    
    # æœ€çµ‚æ¸…ç†
    print("åŸ·è¡Œæœ€çµ‚æ¸…ç†...")
    time.sleep(2)
    kill_chrome_processes()
    
    # æ¨é€åˆ° GitHub
    # push_to_github()

def push_to_github():
    """å°‡æ›´æ–°çš„æª”æ¡ˆæ¨é€åˆ° GitHub"""
    try:
        repo = Repo(os.getcwd())
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´
        if repo.is_dirty() or repo.untracked_files:
            repo.git.add(['data/pvpoke_1500.csv', 'data/pvpoke_2500.csv', 'data/pvpoke_10000.csv'])
            repo.index.commit(f"è‡ªå‹•æ›´æ–° PvP è³‡æ–™ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            origin = repo.remote(name='origin')
            origin.push()
            print("å·²æ¨é€æ›´æ–°åˆ° GitHub")
        else:
            print("æ²’æœ‰æª”æ¡ˆè®Šæ›´ï¼Œè·³é Git æ¨é€")
            
    except Exception as e:
        print(f"Git æ¨é€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()